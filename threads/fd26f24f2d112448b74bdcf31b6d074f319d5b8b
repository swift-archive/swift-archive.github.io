<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Swift Mailing List Archive</title>
    <link rel="stylesheet" href="/css/app-13f065ae5e595562a5022c544e3b976c.css">
  </head>

  <body>
    <div class="container">
      <header class="header">
        <img src="/images/swift-d0237fc716ba0932a940049990beba1b.svg" height="70">
      </header>

      <p class="alert alert-info" role="alert"></p>
      <p class="alert alert-danger" role="alert"></p>

    </div> <!-- /container -->
    <main role="main">
<div class="comment-wrapper"><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/4b3b33f77a1215e7338bfea30585045c?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>Daryle Walker</string> &lt;darylew at mac.com&gt;<p>June 17, 2016 at 04:00:00pm</p></header><div class="content"><p>When I first looked into Swift, I noticed that the base type was called “UInt8” (and “Int8”) and not something like “Byte.”  I know modern computers have followed the bog standard 8/16/32(/64) architecture for decades, but why hard code it into the language/library?  Why should 36-bit processors with 9-bit bytes, or processors that start at 16 bits, be excluded right off the bat?  Did you guys see a problem with how (Objective-)C(++) had to define its base types in a mushy way to accommodate the possibility non-octet bytes?<br></p><p>BTW, is there an equivalent of CHAR_BIT, the number of bits per byte, in the library?  Or are we supposed to hard code an “8”?<br></p><p>— <br>Daryle Walker<br>Mac, Internet, and Video Game Junkie<br>darylew AT mac DOT com <br></p><p>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160617/fd265b8b/attachment.html&gt;<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/2024f9524b1e51a54c4251abf0c34f50?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>Saagar Jha</string> &lt;saagarjha28 at gmail.com&gt;<p>June 17, 2016 at 10:00:00pm</p></header><div class="content"><p>I&#39;m not quite sure what you mean. Swift has a type called Int8 that<br>represents numbers from -128 to 127 using 8 bits. I don&#39;t see how this<br>&quot;excludes&quot; computers.<br></p><p>On Fri, Jun 17, 2016 at 13:01 Daryle Walker via swift-evolution &lt;<br>swift-evolution at swift.org&gt; wrote:<br></p><p>&gt; When I first looked into Swift, I noticed that the base type was called<br>&gt; “UInt8” (and “Int8”) and not something like “Byte.”  I know modern<br>&gt; computers have followed the bog standard 8/16/32(/64) architecture for<br>&gt; decades, but why hard code it into the language/library?  Why should 36-bit<br>&gt; processors with 9-bit bytes, or processors that start at 16 bits, be<br>&gt; excluded right off the bat?  Did you guys see a problem with how<br>&gt; (Objective-)C(++) had to define its base types in a mushy way to<br>&gt; accommodate the possibility non-octet bytes?<br>&gt;<br>&gt; BTW, is there an equivalent of CHAR_BIT, the number of bits per byte, in<br>&gt; the library?  Or are we supposed to hard code an “8”?<br>&gt;<br>&gt; —<br>&gt; Daryle Walker<br>&gt; Mac, Internet, and Video Game Junkie<br>&gt; darylew AT mac DOT com<br>&gt;<br>&gt; _______________________________________________<br>&gt; swift-evolution mailing list<br>&gt; swift-evolution at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>&gt;<br>-- <br>-Saagar Jha<br>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160617/316e421d/attachment.html&gt;<br></p></div></li><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/0e8516d3703e35b7df26986815b23e7a?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>Robert Widmann</string> &lt;devteam.codafi at gmail.com&gt;<p>June 17, 2016 at 08:00:00pm</p></header><div class="content"><p>You raise an interesting point.  To explore this further: we could definitely just lower a lot of it to the appropriate integer-width arithmetic in LLVM.  I suspect the limitations of the standard library implementation you bring up exist because &quot;nonstandard&quot; types such as these don&#39;t show up when we have to bridge C and ObjC so it isn&#39;t as much a priority that we generalize over the entire space.  Doing so would also seem to require the ability to use, say, integer literals in generics like C++.<br></p><p>As for the char size issue, we define both sizeof and a platform-dependent CChar typealias that you can measure against.<br></p><p>~Robert Widmann<br></p><p>2016/06/17 13:01、Daryle Walker via swift-evolution &lt;swift-evolution at swift.org&gt; のメッセージ:<br></p><p>&gt; When I first looked into Swift, I noticed that the base type was called “UInt8” (and “Int8”) and not something like “Byte.”  I know modern computers have followed the bog standard 8/16/32(/64) architecture for decades, but why hard code it into the language/library?  Why should 36-bit processors with 9-bit bytes, or processors that start at 16 bits, be excluded right off the bat?  Did you guys see a problem with how (Objective-)C(++) had to define its base types in a mushy way to accommodate the possibility non-octet bytes?<br>&gt; <br>&gt; BTW, is there an equivalent of CHAR_BIT, the number of bits per byte, in the library?  Or are we supposed to hard code an “8”?<br>&gt; <br>&gt; ― <br>&gt; Daryle Walker<br>&gt; Mac, Internet, and Video Game Junkie<br>&gt; darylew AT mac DOT com <br>&gt; <br>&gt; _______________________________________________<br>&gt; swift-evolution mailing list<br>&gt; swift-evolution at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160617/18c94a2e/attachment.html&gt;<br></p></div></li><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/4b25144c09555e7d5b5e288469e011ef?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>Félix Cloutier</string> &lt;felixcca at yahoo.ca&gt;<p>June 17, 2016 at 08:00:00pm</p></header><div class="content"><p>Out of curiosity, can you name an architecture that doesn&#39;t use 8-bit bytes?<br></p><p>Félix<br></p><p>&gt; Le 17 juin 2016 à 13:01:33, Daryle Walker via swift-evolution &lt;swift-evolution at swift.org&gt; a écrit :<br>&gt; <br>&gt; When I first looked into Swift, I noticed that the base type was called “UInt8” (and “Int8”) and not something like “Byte.”  I know modern computers have followed the bog standard 8/16/32(/64) architecture for decades, but why hard code it into the language/library?  Why should 36-bit processors with 9-bit bytes, or processors that start at 16 bits, be excluded right off the bat?  Did you guys see a problem with how (Objective-)C(++) had to define its base types in a mushy way to accommodate the possibility non-octet bytes?<br>&gt; <br>&gt; BTW, is there an equivalent of CHAR_BIT, the number of bits per byte, in the library?  Or are we supposed to hard code an “8”?<br>&gt; <br>&gt; — <br>&gt; Daryle Walker<br>&gt; Mac, Internet, and Video Game Junkie<br>&gt; darylew AT mac DOT com <br>&gt; <br>&gt; _______________________________________________<br>&gt; swift-evolution mailing list<br>&gt; swift-evolution at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br></p><p>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160617/3e5b079c/attachment.html&gt;<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/1f823d615b521ed15f1006b105c77900?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>David Sweeris</string> &lt;davesweeris at mac.com&gt;<p>June 17, 2016 at 11:00:00pm</p></header><div class="content"><p>IIRC, a bunch of Ye Olde systems used 6-bit bytes. And I think 36-bit ints were used in a few architectures, but don&#39;t quote me on that.<br></p><p>- Dave Sweeris<br></p><p>&gt; On Jun 17, 2016, at 22:48, Félix Cloutier via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt; <br>&gt; Out of curiosity, can you name an architecture that doesn&#39;t use 8-bit bytes?<br>&gt; <br>&gt; Félix<br>&gt; <br>&gt;&gt; Le 17 juin 2016 à 13:01:33, Daryle Walker via swift-evolution &lt;swift-evolution at swift.org&gt; a écrit :<br>&gt;&gt; <br>&gt;&gt; When I first looked into Swift, I noticed that the base type was called “UInt8” (and “Int8”) and not something like “Byte.”  I know modern computers have followed the bog standard 8/16/32(/64) architecture for decades, but why hard code it into the language/library?  Why should 36-bit processors with 9-bit bytes, or processors that start at 16 bits, be excluded right off the bat?  Did you guys see a problem with how (Objective-)C(++) had to define its base types in a mushy way to accommodate the possibility non-octet bytes?<br>&gt;&gt; <br>&gt;&gt; BTW, is there an equivalent of CHAR_BIT, the number of bits per byte, in the library?  Or are we supposed to hard code an “8”?<br>&gt;&gt; <br>&gt;&gt; — <br>&gt;&gt; Daryle Walker<br>&gt;&gt; Mac, Internet, and Video Game Junkie<br>&gt;&gt; darylew AT mac DOT com <br>&gt;&gt; <br>&gt;&gt; _______________________________________________<br>&gt;&gt; swift-evolution mailing list<br>&gt;&gt; swift-evolution at swift.org<br>&gt;&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>&gt; <br>&gt; _______________________________________________<br>&gt; swift-evolution mailing list<br>&gt; swift-evolution at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160617/f187417f/attachment.html&gt;<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/0e8516d3703e35b7df26986815b23e7a?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>Robert Widmann</string> &lt;devteam.codafi at gmail.com&gt;<p>June 17, 2016 at 09:00:00pm</p></header><div class="content"><p>Old old old architectures.  We&#39;re talking Multics days.<br></p><p>~Robert Widmann<br></p><p>2016/06/17 21:35、David Sweeris via swift-evolution &lt;swift-evolution at swift.org&gt; のメッセージ:<br></p><p>&gt; IIRC, a bunch of Ye Olde systems used 6-bit bytes. And I think 36-bit ints were used in a few architectures, but don&#39;t quote me on that.<br>&gt; <br>&gt; - Dave Sweeris<br>&gt; <br>&gt;&gt; On Jun 17, 2016, at 22:48, Félix Cloutier via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt;&gt; <br>&gt;&gt; Out of curiosity, can you name an architecture that doesn&#39;t use 8-bit bytes?<br>&gt;&gt; <br>&gt;&gt; Félix<br>&gt;&gt; <br>&gt;&gt;&gt; Le 17 juin 2016 à 13:01:33, Daryle Walker via swift-evolution &lt;swift-evolution at swift.org&gt; a écrit :<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; When I first looked into Swift, I noticed that the base type was called “UInt8” (and “Int8”) and not something like “Byte.”  I know modern computers have followed the bog standard 8/16/32(/64) architecture for decades, but why hard code it into the language/library?  Why should 36-bit processors with 9-bit bytes, or processors that start at 16 bits, be excluded right off the bat?  Did you guys see a problem with how (Objective-)C(++) had to define its base types in a mushy way to accommodate the possibility non-octet bytes?<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; BTW, is there an equivalent of CHAR_BIT, the number of bits per byte, in the library?  Or are we supposed to hard code an “8”?<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; — <br>&gt;&gt;&gt; Daryle Walker<br>&gt;&gt;&gt; Mac, Internet, and Video Game Junkie<br>&gt;&gt;&gt; darylew AT mac DOT com <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; _______________________________________________<br>&gt;&gt;&gt; swift-evolution mailing list<br>&gt;&gt;&gt; swift-evolution at swift.org<br>&gt;&gt;&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>&gt;&gt; <br>&gt;&gt; _______________________________________________<br>&gt;&gt; swift-evolution mailing list<br>&gt;&gt; swift-evolution at swift.org<br>&gt;&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>&gt; _______________________________________________<br>&gt; swift-evolution mailing list<br>&gt; swift-evolution at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160617/6c023851/attachment.html&gt;<br></p></div></li></ul></li></ul></li><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/09b6a9b66eecf177910c8e47db78d8eb?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>Chris Lattner</string> &lt;clattner at apple.com&gt;<p>June 18, 2016 at 10:00:00pm</p></header><div class="content"><p>&gt; On Jun 17, 2016, at 1:01 PM, Daryle Walker via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt; <br>&gt; When I first looked into Swift, I noticed that the base type was called “UInt8” (and “Int8”) and not something like “Byte.”  I know modern computers have followed the bog standard 8/16/32(/64) architecture for decades, but why hard code it into the language/library?  Why should 36-bit processors with 9-bit bytes, or processors that start at 16 bits, be excluded right off the bat?  Did you guys see a problem with how (Objective-)C(++) had to define its base types in a mushy way to accommodate the possibility non-octet bytes?<br></p><p>Given that there are no 9-bit byte targets supported by Swift (or LLVM), it would be impossible to test that configuration, and it is well known that untested code doesn’t work.  As such, introducing a Byte type which is potentially not 8 bits in size would only add cognitive overload.  Any promised portability benefit would simply mislead people.<br></p><p>-Chris<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/4b3b33f77a1215e7338bfea30585045c?s=50"></div><header><strong>Why hard-code octet-sized bytes?</strong> from <string>Daryle Walker</string> &lt;darylew at mac.com&gt;<p>June 19, 2016 at 06:00:00pm</p></header><div class="content"><p>&gt; On Jun 19, 2016, at 1:04 AM, Chris Lattner &lt;clattner at apple.com&gt; wrote:<br>&gt; <br>&gt;&gt; On Jun 17, 2016, at 1:01 PM, Daryle Walker via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt;&gt; <br>&gt;&gt; When I first looked into Swift, I noticed that the base type was called “UInt8” (and “Int8”) and not something like “Byte.”  I know modern computers have followed the bog standard 8/16/32(/64) architecture for decades, but why hard code it into the language/library?  Why should 36-bit processors with 9-bit bytes, or processors that start at 16 bits, be excluded right off the bat?  Did you guys see a problem with how (Objective-)C(++) had to define its base types in a mushy way to accommodate the possibility non-octet bytes?<br>&gt; <br>&gt; Given that there are no 9-bit byte targets supported by Swift (or LLVM), it would be impossible to test that configuration, and it is well known that untested code doesn’t work.  As such, introducing a Byte type which is potentially not 8 bits in size would only add cognitive overload.  Any promised portability benefit would simply mislead people.<br></p><p>I wasn’t proposing adding “Byte,” etc. now, but asking why the originators didn’t go for width-agnostic names for the base numeric types way back when INSTEAD of what we do have.  I know that nowadays any violators would be embedded systems that start off using 16- or 32-bits for everything.  We’re more likely to go to 128 bits than any non-power-of-2 in the future.<br></p><p>(I don’t know what names we would use besides the “Byte” and “Int” ones.  Since the “Int” type is supposed to move up with processor improvements, I guess we should go with “Short,” “ShortShort,” etc. instead of multiple “long”s like C++ did.)<br></p><p>If we had value-based generic arguments, we could have had something like as set of “Integer&lt;width: Int&gt;”, with “exact” and “at least” variants.  We would still need “Byte” and “Int” unless we provide constants for the environment’s minimum and optimized bit widths (and even then, the type aliases would be handy).<br></p><p>— <br>Daryle Walker<br>Mac, Internet, and Video Game Junkie<br>darylew AT mac DOT com<br></p></div></li></ul></li></ul></li></ul></div>    </main>
    <script src="/js/app-c283ee129de63ad743722e9511e67a5d.js"></script>
  </body>
  <footer>
    <p>Swift and the Swift logo are trademarks of Apple Inc.</p>
  </footer>
</html>
