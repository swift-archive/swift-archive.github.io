<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Swift Mailing List Archive</title>
    <link rel="stylesheet" href="/css/app-13f065ae5e595562a5022c544e3b976c.css?vsn=d">
  </head>

  <body>
    <div class="container">
      <header class="header">
        <img src="/images/swift-d0237fc716ba0932a940049990beba1b.svg?vsn=d" height="70">
      </header>

      <p class="alert alert-info" role="alert"></p>
      <p class="alert alert-danger" role="alert"></p>

    </div> <!-- /container -->
    <main role="main">
<div class="comment-wrapper"><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/a838c63b9093a77fb88231d65effa3b3?s=50"></div><header><strong>[xctest] Removing outliers from performance tests</strong> from <string>Drew Crawford</string> &lt;drew at sealedabstract.com&gt;<p>December 10, 2015 at 05:00:00am</p></header><div class="content"><p>Hello folks,<br></p><p>I’m one of the heavy users of XCTest.measureBlock as it exists in Xcode 7.2.  To give some hard numbers, I have ~50 performance tests in an OSX framework project, occupying about 20m wall clock time total.  This occurs on a per-commit basis.<br></p><p>The current implementation of measureBlock as it currently exists in closed-source Xcode is something like this:<br></p><p>1.  Run 10 trials<br>2.  Compare the average across those 10 trials to some baseline<br>3.  Compare the stdev across those 10 trials to some standard value (10% by default)<br></p><p>There are really a lot of problems with this algorithm, but maybe the biggest one is how it handles outliers.  If you have a test suite running for 20m, chances are “something” is going to happen on the build server in that time.  System background task, software update, gremlins etc.<br></p><p>So what happens lately is exactly *one* of the 10 * 50 = 500 total measureBlocks takes a really long time, and it is a different failure each time (e.g., it’s not my code, I swear).  A result like this for some test is typical:<br></p><p><br></p><p><br>The probability of this kind of error grows exponentially with the test suite size.  If we assume for an individual measureBlock that it only fails due to “chance” .01% of the time, then the overall test suite at N = 500 will only pass 60% of the time.  This is very vaguely consistent with what I experience at my scale—e.g. a test suite that does not really tell me if my code is broken or not.<br></p><p>IMO the problem here is one of experiment design.  From the data in the screenshot, this very well might be a real performance regression that should be properly investigated.  It is only when I tell you a lot of extra information—e.g. that this test will pass fine the next 100 executions and it’s part of an enormous test suite where something is bound to fail—that a failure due to random chance seems likely.  In other words, running 10 iterations and pretending that will find performance regressions is a poor approach.<br></p><p>I’ve done some prototyping on algorithms that use a dynamically sized number of trials to find performance regressions.  Apple employees, see rdar://21315474 for an algorithm for a sliding window for performance tests (that also has other benefits, like measuring nanosecond-scale performance).  I am certainly willing to contrib that work in the open if there’s consensus it’s a good direction.<br></p><p>However, now that this is happening in the open, I’m interested in getting others’ thoughts on this problem.  Surely I am not the only serious user of performance tests, and maybe people with better statistics backgrounds than I have can suggest an appropriate solution.<br></p><p>Drew<br>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-corelibs-dev/attachments/20151210/a4056226/attachment.html&gt;<br>-------------- next part --------------<br>A non-text attachment was scrubbed...<br>Name: Screen Shot 2015-12-10 at 5.12.13 AM.png<br>Type: image/png<br>Size: 87121 bytes<br>Desc: not available<br>URL: &lt;https://lists.swift.org/pipermail/swift-corelibs-dev/attachments/20151210/a4056226/attachment.png&gt;<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/be4c3c3b76631a155e1358db48066692?s=50"></div><header><strong>[xctest] Removing outliers from performance tests</strong> from <string>Tony Parker</string> &lt;anthony.parker at apple.com&gt;<p>December 12, 2015 at 05:00:00pm</p></header><div class="content"><p>Hi Drew,<br></p><p>Thanks for the detailed info on your issue. I see you filed a radar, and that is indeed the best way to make sure an issue on Darwin platforms is addressed. Unfortunately our corelibs implementation of XCTest isn’t ready yet for performance testing.<br></p><p>- Tony<br></p><p>&gt; On Dec 10, 2015, at 3:41 AM, Drew Crawford via swift-corelibs-dev &lt;swift-corelibs-dev at swift.org&gt; wrote:<br>&gt; <br>&gt; Hello folks,<br>&gt; <br>&gt; I’m one of the heavy users of XCTest.measureBlock as it exists in Xcode 7.2.  To give some hard numbers, I have ~50 performance tests in an OSX framework project, occupying about 20m wall clock time total.  This occurs on a per-commit basis.<br>&gt; <br>&gt; The current implementation of measureBlock as it currently exists in closed-source Xcode is something like this:<br>&gt; <br>&gt; 1.  Run 10 trials<br>&gt; 2.  Compare the average across those 10 trials to some baseline<br>&gt; 3.  Compare the stdev across those 10 trials to some standard value (10% by default)<br>&gt; <br>&gt; There are really a lot of problems with this algorithm, but maybe the biggest one is how it handles outliers.  If you have a test suite running for 20m, chances are “something” is going to happen on the build server in that time.  System background task, software update, gremlins etc.<br>&gt; <br>&gt; So what happens lately is exactly *one* of the 10 * 50 = 500 total measureBlocks takes a really long time, and it is a different failure each time (e.g., it’s not my code, I swear).  A result like this for some test is typical:<br>&gt; <br>&gt; &lt;Screen Shot 2015-12-10 at 5.12.13 AM.png&gt;<br>&gt; <br>&gt; <br>&gt; The probability of this kind of error grows exponentially with the test suite size.  If we assume for an individual measureBlock that it only fails due to “chance” .01% of the time, then the overall test suite at N = 500 will only pass 60% of the time.  This is very vaguely consistent with what I experience at my scale—e.g. a test suite that does not really tell me if my code is broken or not.<br>&gt; <br>&gt; IMO the problem here is one of experiment design.  From the data in the screenshot, this very well might be a real performance regression that should be properly investigated.  It is only when I tell you a lot of extra information—e.g. that this test will pass fine the next 100 executions and it’s part of an enormous test suite where something is bound to fail—that a failure due to random chance seems likely.  In other words, running 10 iterations and pretending that will find performance regressions is a poor approach.<br>&gt; <br>&gt; I’ve done some prototyping on algorithms that use a dynamically sized number of trials to find performance regressions.  Apple employees, see rdar://21315474 &lt;rdar://21315474&gt; for an algorithm for a sliding window for performance tests (that also has other benefits, like measuring nanosecond-scale performance).  I am certainly willing to contrib that work in the open if there’s consensus it’s a good direction.<br>&gt; <br>&gt; However, now that this is happening in the open, I’m interested in getting others’ thoughts on this problem.  Surely I am not the only serious user of performance tests, and maybe people with better statistics backgrounds than I have can suggest an appropriate solution.<br>&gt; <br>&gt; Drew<br>&gt; <br>&gt; _______________________________________________<br>&gt; swift-corelibs-dev mailing list<br>&gt; swift-corelibs-dev at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-corelibs-dev<br></p><p>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-corelibs-dev/attachments/20151212/c4646f54/attachment.html&gt;<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/a838c63b9093a77fb88231d65effa3b3?s=50"></div><header><strong>[xctest] Removing outliers from performance tests</strong> from <string>Drew Crawford</string> &lt;drew at sealedabstract.com&gt;<p>December 12, 2015 at 08:00:00pm</p></header><div class="content"><p>&gt; Unfortunately our corelibs implementation of XCTest isn’t ready yet for performance testing.<br></p><p>That&#39;s why I&#39;m here; I&#39;m taking the temperature on implementing it.  I&#39;m at the pain level where I need a solution in the next several months, even if the solution is to code it up myself.  My tests have failed 10x over this so far today.<br></p><p>I think the real question is, if I did implement basic performance testing, and I did implement a variable-sized window of runs, would that departure from the Old XCTest behavior (which uses 10 runs) disqualify the PR?  It&#39;s a basic compatibility question about how close we need to follow the Old XCTest behavior.<br></p><p>e.g. if XCS wanted to migrate to corelibs-xctest and it used variable #s of runs, presumably that would be an undertaking for the XCS team.  But I don&#39;t know whether those concerns (if they exist) play a role in what this project decides to do.<br></p><p>I&#39;m going to do something on this problem eventually, unless someone else solves it first.  I&#39;m just trying to work out whether I can do something upstream or whether this is a better candidate for an independent effort.<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/6bab59a4fee6a3c2697717acb34d647c?s=50"></div><header><strong>[xctest] Removing outliers from performance tests</strong> from <string>Mike Ferris</string> &lt;mferris at apple.com&gt;<p>December 13, 2015 at 07:00:00am</p></header><div class="content"><p>I think that there’s a lot of room for improvement in how we measure and analyze perf test results. And I like some of your ideas of adaptive numbers of runs and what to do with outlier results, etc… that you presented below.<br></p><p>We would very much like to not diverge the API of the CoreLibs XCTest and Xcode’s.<br></p><p>There may be a bit more room for some variation in the behavior. So talking about doing something where the number of runs varies dynamically not via API but through smarter execution mechanisms, or doing better statistical analysis such as removing outlier results, this would introduce some change in the results that might be reported between the two implementations, but it would not give rise to people writing tests that would not work cross-platform.<br></p><p>Further, there’s always room for discussion of taking such ideas, and even API additions and supporting them in Xcode’s XCTest as well. But that sort of discussion will have to include the practicalities of schedule and resource on the Xcode side (and, to some unavoidable extent, there will be aspects to that discussion that will not be as transparent since we’ll be weighing that work against other work for the testing team that cannot be discussed as freely.)<br></p><p>This year, our primary goal for the core libraries is to broaden the implementation of the existing APIs in the OS X versions of the frameworks. For XCTest, we would also love to come up with some better answer for test discovery and potentially elicit help from the community in achieving that. Beyond that, the Xcode team’s bandwidth for incorporating other things that would necessaitate change to the Xcode XCTest is going to be limited.<br></p><p>Mike<br></p><p><br>&gt; On Dec 12, 2015, at 6:10 PM, Drew Crawford via swift-corelibs-dev &lt;swift-corelibs-dev at swift.org&gt; wrote:<br>&gt; <br>&gt;&gt; Unfortunately our corelibs implementation of XCTest isn’t ready yet for performance testing.<br>&gt; <br>&gt; That&#39;s why I&#39;m here; I&#39;m taking the temperature on implementing it.  I&#39;m at the pain level where I need a solution in the next several months, even if the solution is to code it up myself.  My tests have failed 10x over this so far today.<br>&gt; <br>&gt; I think the real question is, if I did implement basic performance testing, and I did implement a variable-sized window of runs, would that departure from the Old XCTest behavior (which uses 10 runs) disqualify the PR?  It&#39;s a basic compatibility question about how close we need to follow the Old XCTest behavior.<br>&gt; <br>&gt; e.g. if XCS wanted to migrate to corelibs-xctest and it used variable #s of runs, presumably that would be an undertaking for the XCS team.  But I don&#39;t know whether those concerns (if they exist) play a role in what this project decides to do.<br>&gt; <br>&gt; I&#39;m going to do something on this problem eventually, unless someone else solves it first.  I&#39;m just trying to work out whether I can do something upstream or whether this is a better candidate for an independent effort.<br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; <br>&gt; _______________________________________________<br>&gt; swift-corelibs-dev mailing list<br>&gt; swift-corelibs-dev at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-corelibs-dev<br></p></div></li></ul></li></ul></li></ul></li></ul></div>    </main>
    <script src="/js/app-c283ee129de63ad743722e9511e67a5d.js?vsn=d"></script>
  </body>
  <footer>
    <p>Swift and the Swift logo are trademarks of Apple Inc.</p>
  </footer>
</html>
