<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Swift Mailing List Archive</title>
    <link rel="stylesheet" href="/css/app-13f065ae5e595562a5022c544e3b976c.css">
  </head>

  <body>
    <div class="container">
      <header class="header">
        <img src="/images/swift-d0237fc716ba0932a940049990beba1b.svg" height="70">
      </header>

      <p class="alert alert-info" role="alert"></p>
      <p class="alert alert-danger" role="alert"></p>

    </div> <!-- /container -->
    <main role="main">
<div class="comment-wrapper"><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/b2f47d083afa4931ca8897e95f0cb4ab?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Morten Bek Ditlevsen</string> &lt;bek at termestrup.dk&gt;<p>May  6, 2016 at 11:00:00am</p></header><div class="content"><p>Currently, in order to conform to FloatLiteralConvertible you need to implement<br>an initializer accepting a floatLiteral of the typealias: FloatLiteralType.<br>However, this typealias can only be Double, Float, Float80 and other built-in<br>floating point types (to be honest, I do not know the exact limitation since I have<br>not been able to read find this in the documentation).<br></p><p>These floating point types have precision limitations that are not necessarily<br>present in the type that you are making FloatLiteralConvertible.<br></p><p>Let’s imagine a CurrencyAmount type that uses an NSDecimalNumber as the<br>representation of the value:<br></p><p><br>public struct CurrencyAmount {<br>  public let value: NSDecimalNumber <br>  // .. other important currency-related stuff ..<br>}<br></p><p>extension CurrencyAmount: FloatLiteralConvertible { <br>  public typealias FloatLiteralType = Double<br>    <br>  public init(floatLiteral amount: FloatLiteralType) {<br>    print(amount.debugDescription) <br>    value = NSDecimalNumber(double: amount) <br>  } <br>}<br></p><p>let a: CurrencyAmount = 99.99<br></p><p><br>The printed value inside the initializer is 99.989999999999995 - so the value<br>has lost precision already in the intermediary Double representation.  <br></p><p>I know that there is also an issue with the NSDecimalNumber double initializer,<br>but this is not the issue that we are seeing here.<br></p><p><br>One suggestion for a solution to this issue would be to allow the<br>FloatLiteralType to be aliased to a String.  In this case the compiler should<br>parse the float literal token: 99.99 to a String and use that as input for the<br>FloatLiteralConvertible initializer.<br></p><p>This would mean that arbitrary literal precisions are allowed for<br>FloatLiteralConvertibles that implement their own parsing of a String value.<br></p><p>For instance, if the CurrencyAmount used a FloatLiteralType aliased to String we<br>would have:<br></p><p>extension CurrencyAmount: FloatLiteralConvertible { <br>  public typealias FloatLiteralType = String<br>    <br>  public init(floatLiteral amount: FloatLiteralType) { <br>    value = NSDecimalNumber(string: amount) <br>  } <br>}<br></p><p>and the precision would be the same as creating an NSDecimalNumber from a<br>String: <br></p><p>let a: CurrencyAmount = 1.00000000000000000000000000000000001<br></p><p>print(a.value.debugDescription)<br></p><p>Would give: 1.00000000000000000000000000000000001<br></p><p><br>How does that sound? Is it completely irrational to allow the use of Strings as<br>the intermediary representation of float literals?<br>I think that it makes good sense, since it allows for arbitrary precision.<br></p><p>Please let me know what you think.<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/969b04d28c02f951ccc03f615b9a92b3?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Dmitri Gribenko</string> &lt;gribozavr at gmail.com&gt;<p>May  6, 2016 at 09:00:00am</p></header><div class="content"><p>On Fri, May 6, 2016 at 2:24 AM, Morten Bek Ditlevsen via<br>swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt; How does that sound? Is it completely irrational to allow the use of Strings as<br>&gt; the intermediary representation of float literals?<br>&gt; I think that it makes good sense, since it allows for arbitrary precision.<br></p><p>Hi,<br></p><p>I think you are raising an important problem, but using String as the<br>intermediate type does not strike me as an efficient and clean<br>solution.  We already have DictionaryLiteral, and extending that<br>family to also include FloatLiteral seems like the right direction to<br>me.<br></p><p>Dmitri<br></p><p>-- <br>main(i,j){for(i=2;;i++){for(j=2;j&lt;i;j++){if(!(i%j)){j=0;break;}}if<br>(j){printf(&quot;%d\n&quot;,i);}}} /*Dmitri Gribenko &lt;gribozavr at gmail.com&gt;*/<br></p></div></li><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/06c1dcc7dce6a93d194b9c013376e3f0?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Joe Groff</string> &lt;jgroff at apple.com&gt;<p>May  6, 2016 at 09:00:00am</p></header><div class="content"><p>&gt; On May 6, 2016, at 2:24 AM, Morten Bek Ditlevsen via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt; <br>&gt; Currently, in order to conform to FloatLiteralConvertible you need to implement<br>&gt; an initializer accepting a floatLiteral of the typealias: FloatLiteralType.<br>&gt; However, this typealias can only be Double, Float, Float80 and other built-in<br>&gt; floating point types (to be honest, I do not know the exact limitation since I have<br>&gt; not been able to read find this in the documentation).<br>&gt; <br>&gt; These floating point types have precision limitations that are not necessarily<br>&gt; present in the type that you are making FloatLiteralConvertible.<br>&gt; <br>&gt; Let’s imagine a CurrencyAmount type that uses an NSDecimalNumber as the<br>&gt; representation of the value:<br>&gt; <br>&gt; <br>&gt; public struct CurrencyAmount {<br>&gt;  public let value: NSDecimalNumber <br>&gt;  // .. other important currency-related stuff ..<br>&gt; }<br>&gt; <br>&gt; extension CurrencyAmount: FloatLiteralConvertible { <br>&gt;  public typealias FloatLiteralType = Double<br>&gt; <br>&gt;  public init(floatLiteral amount: FloatLiteralType) {<br>&gt;    print(amount.debugDescription) <br>&gt;    value = NSDecimalNumber(double: amount) <br>&gt;  } <br>&gt; }<br>&gt; <br>&gt; let a: CurrencyAmount = 99.99<br>&gt; <br>&gt; <br>&gt; The printed value inside the initializer is 99.989999999999995 - so the value<br>&gt; has lost precision already in the intermediary Double representation.  <br>&gt; <br>&gt; I know that there is also an issue with the NSDecimalNumber double initializer,<br>&gt; but this is not the issue that we are seeing here.<br>&gt; <br>&gt; <br>&gt; One suggestion for a solution to this issue would be to allow the<br>&gt; FloatLiteralType to be aliased to a String.  In this case the compiler should<br>&gt; parse the float literal token: 99.99 to a String and use that as input for the<br>&gt; FloatLiteralConvertible initializer.<br>&gt; <br>&gt; This would mean that arbitrary literal precisions are allowed for<br>&gt; FloatLiteralConvertibles that implement their own parsing of a String value.<br>&gt; <br>&gt; For instance, if the CurrencyAmount used a FloatLiteralType aliased to String we<br>&gt; would have:<br>&gt; <br>&gt; extension CurrencyAmount: FloatLiteralConvertible { <br>&gt;  public typealias FloatLiteralType = String<br>&gt; <br>&gt;  public init(floatLiteral amount: FloatLiteralType) { <br>&gt;    value = NSDecimalNumber(string: amount) <br>&gt;  } <br>&gt; }<br>&gt; <br>&gt; and the precision would be the same as creating an NSDecimalNumber from a<br>&gt; String: <br>&gt; <br>&gt; let a: CurrencyAmount = 1.00000000000000000000000000000000001<br>&gt; <br>&gt; print(a.value.debugDescription)<br>&gt; <br>&gt; Would give: 1.00000000000000000000000000000000001<br>&gt; <br>&gt; <br>&gt; How does that sound? Is it completely irrational to allow the use of Strings as<br>&gt; the intermediary representation of float literals?<br>&gt; I think that it makes good sense, since it allows for arbitrary precision.<br>&gt; <br>&gt; Please let me know what you think.<br></p><p>Like Dmitri said, a String is not a particularly efficient intermediate representation. For common machine numeric types, we want it to be straightforward for the compiler to constant-fold literals down to constants in the resulting binary. For floating-point literals, I think we could achieve this by changing the protocol to &quot;deconstruct&quot; the literal value into integer significand and exponent, something like this:<br></p><p>// A type that can be initialized from a decimal literal such as<br>// `1.1` or `2.3e5`.<br>protocol DecimalLiteralConvertible {<br>  // The integer type used to represent the significand and exponent of the value.<br>  typealias Component: IntegerLiteralConvertible<br></p><p>  // Construct a value equal to `decimalSignificand * 10**decimalExponent`.<br>  init(decimalSignificand: Component, decimalExponent: Component)<br>}<br></p><p>// A type that can be initialized from a hexadecimal floating point<br>// literal, such as `0x1.8p-2`.<br>protocol HexFloatLiteralConvertible {<br>  // The integer type used to represent the significand and exponent of the value.<br>  typealias Component: IntegerLiteralConvertible<br></p><p>  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent`.<br>  init(hexadecimalSignificand: Component, binaryExponent: Component)<br>}<br></p><p>Literals would desugar to constructor calls as follows:<br></p><p>1.0 // T(decimalSignificand: 1, decimalExponent: 0)<br>0.123 // T(decimalSignificand: 123, decimalExponent: -3)<br>1.23e-2 // same<br></p><p>0x1.8p-2 // T(hexadecimalSignificand: 0x18, binaryExponent: -6)<br></p><p>-Joe<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/d9b7c9b023237138ccb67df539f11b50?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Stephen Canon</string> &lt;scanon at apple.com&gt;<p>May  6, 2016 at 12:00:00pm</p></header><div class="content"><p>&gt; On May 6, 2016, at 12:41 PM, Joe Groff via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt; <br>&gt;&gt; <br>&gt;&gt; On May 6, 2016, at 2:24 AM, Morten Bek Ditlevsen via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt;&gt; <br>&gt;&gt; Currently, in order to conform to FloatLiteralConvertible you need to implement<br>&gt;&gt; an initializer accepting a floatLiteral of the typealias: FloatLiteralType.<br>&gt;&gt; However, this typealias can only be Double, Float, Float80 and other built-in<br>&gt;&gt; floating point types (to be honest, I do not know the exact limitation since I have<br>&gt;&gt; not been able to read find this in the documentation).<br>&gt;&gt; <br>&gt;&gt; These floating point types have precision limitations that are not necessarily<br>&gt;&gt; present in the type that you are making FloatLiteralConvertible.<br>&gt;&gt; <br>&gt;&gt; Let’s imagine a CurrencyAmount type that uses an NSDecimalNumber as the<br>&gt;&gt; representation of the value:<br>&gt;&gt; <br>&gt;&gt; <br>&gt;&gt; public struct CurrencyAmount {<br>&gt;&gt; public let value: NSDecimalNumber <br>&gt;&gt; // .. other important currency-related stuff ..<br>&gt;&gt; }<br>&gt;&gt; <br>&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible { <br>&gt;&gt; public typealias FloatLiteralType = Double<br>&gt;&gt; <br>&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt;&gt;   print(amount.debugDescription) <br>&gt;&gt;   value = NSDecimalNumber(double: amount) <br>&gt;&gt; } <br>&gt;&gt; }<br>&gt;&gt; <br>&gt;&gt; let a: CurrencyAmount = 99.99<br>&gt;&gt; <br>&gt;&gt; <br>&gt;&gt; The printed value inside the initializer is 99.989999999999995 - so the value<br>&gt;&gt; has lost precision already in the intermediary Double representation.  <br>&gt;&gt; <br>&gt;&gt; I know that there is also an issue with the NSDecimalNumber double initializer,<br>&gt;&gt; but this is not the issue that we are seeing here.<br>&gt;&gt; <br>&gt;&gt; <br>&gt;&gt; One suggestion for a solution to this issue would be to allow the<br>&gt;&gt; FloatLiteralType to be aliased to a String.  In this case the compiler should<br>&gt;&gt; parse the float literal token: 99.99 to a String and use that as input for the<br>&gt;&gt; FloatLiteralConvertible initializer.<br>&gt;&gt; <br>&gt;&gt; This would mean that arbitrary literal precisions are allowed for<br>&gt;&gt; FloatLiteralConvertibles that implement their own parsing of a String value.<br>&gt;&gt; <br>&gt;&gt; For instance, if the CurrencyAmount used a FloatLiteralType aliased to String we<br>&gt;&gt; would have:<br>&gt;&gt; <br>&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible { <br>&gt;&gt; public typealias FloatLiteralType = String<br>&gt;&gt; <br>&gt;&gt; public init(floatLiteral amount: FloatLiteralType) { <br>&gt;&gt;   value = NSDecimalNumber(string: amount) <br>&gt;&gt; } <br>&gt;&gt; }<br>&gt;&gt; <br>&gt;&gt; and the precision would be the same as creating an NSDecimalNumber from a<br>&gt;&gt; String: <br>&gt;&gt; <br>&gt;&gt; let a: CurrencyAmount = 1.00000000000000000000000000000000001<br>&gt;&gt; <br>&gt;&gt; print(a.value.debugDescription)<br>&gt;&gt; <br>&gt;&gt; Would give: 1.00000000000000000000000000000000001<br>&gt;&gt; <br>&gt;&gt; <br>&gt;&gt; How does that sound? Is it completely irrational to allow the use of Strings as<br>&gt;&gt; the intermediary representation of float literals?<br>&gt;&gt; I think that it makes good sense, since it allows for arbitrary precision.<br>&gt;&gt; <br>&gt;&gt; Please let me know what you think.<br>&gt; <br>&gt; Like Dmitri said, a String is not a particularly efficient intermediate representation. For common machine numeric types, we want it to be straightforward for the compiler to constant-fold literals down to constants in the resulting binary. For floating-point literals, I think we could achieve this by changing the protocol to &quot;deconstruct&quot; the literal value into integer significand and exponent, something like this:<br>&gt; <br>&gt; // A type that can be initialized from a decimal literal such as<br>&gt; // `1.1` or `2.3e5`.<br>&gt; protocol DecimalLiteralConvertible {<br>&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; <br>&gt;  // Construct a value equal to `decimalSignificand * 10**decimalExponent`.<br>&gt;  init(decimalSignificand: Component, decimalExponent: Component)<br>&gt; }<br>&gt; <br>&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt; // literal, such as `0x1.8p-2`.<br>&gt; protocol HexFloatLiteralConvertible {<br>&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; <br>&gt;  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent`.<br>&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component)<br>&gt; }<br>&gt; <br>&gt; Literals would desugar to constructor calls as follows:<br>&gt; <br>&gt; 1.0 // T(decimalSignificand: 1, decimalExponent: 0)<br>&gt; 0.123 // T(decimalSignificand: 123, decimalExponent: -3)<br>&gt; 1.23e-2 // same<br>&gt; <br>&gt; 0x1.8p-2 // T(hexadecimalSignificand: 0x18, binaryExponent: -6)<br></p><p>This seems like a very good approach to me.<br></p><p>– Steve<br></p><p>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160506/5036106c/attachment-0001.html&gt;<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/06c1dcc7dce6a93d194b9c013376e3f0?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Joe Groff</string> &lt;jgroff at apple.com&gt;<p>May  6, 2016 at 09:00:00am</p></header><div class="content"><p>&gt; On May 6, 2016, at 9:42 AM, Stephen Canon &lt;scanon at apple.com&gt; wrote:<br>&gt; <br>&gt; <br>&gt;&gt; On May 6, 2016, at 12:41 PM, Joe Groff via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt;&gt; <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; On May 6, 2016, at 2:24 AM, Morten Bek Ditlevsen via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; Currently, in order to conform to FloatLiteralConvertible you need to implement<br>&gt;&gt;&gt; an initializer accepting a floatLiteral of the typealias: FloatLiteralType.<br>&gt;&gt;&gt; However, this typealias can only be Double, Float, Float80 and other built-in<br>&gt;&gt;&gt; floating point types (to be honest, I do not know the exact limitation since I have<br>&gt;&gt;&gt; not been able to read find this in the documentation).<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; These floating point types have precision limitations that are not necessarily<br>&gt;&gt;&gt; present in the type that you are making FloatLiteralConvertible.<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; Let’s imagine a CurrencyAmount type that uses an NSDecimalNumber as the<br>&gt;&gt;&gt; representation of the value:<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; public struct CurrencyAmount {<br>&gt;&gt;&gt; public let value: NSDecimalNumber <br>&gt;&gt;&gt; // .. other important currency-related stuff ..<br>&gt;&gt;&gt; }<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible { <br>&gt;&gt;&gt; public typealias FloatLiteralType = Double<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt;&gt;&gt;   print(amount.debugDescription) <br>&gt;&gt;&gt;   value = NSDecimalNumber(double: amount) <br>&gt;&gt;&gt; } <br>&gt;&gt;&gt; }<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; let a: CurrencyAmount = 99.99<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; The printed value inside the initializer is 99.989999999999995 - so the value<br>&gt;&gt;&gt; has lost precision already in the intermediary Double representation.  <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; I know that there is also an issue with the NSDecimalNumber double initializer,<br>&gt;&gt;&gt; but this is not the issue that we are seeing here.<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; One suggestion for a solution to this issue would be to allow the<br>&gt;&gt;&gt; FloatLiteralType to be aliased to a String.  In this case the compiler should<br>&gt;&gt;&gt; parse the float literal token: 99.99 to a String and use that as input for the<br>&gt;&gt;&gt; FloatLiteralConvertible initializer.<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; This would mean that arbitrary literal precisions are allowed for<br>&gt;&gt;&gt; FloatLiteralConvertibles that implement their own parsing of a String value.<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; For instance, if the CurrencyAmount used a FloatLiteralType aliased to String we<br>&gt;&gt;&gt; would have:<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible { <br>&gt;&gt;&gt; public typealias FloatLiteralType = String<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) { <br>&gt;&gt;&gt;   value = NSDecimalNumber(string: amount) <br>&gt;&gt;&gt; } <br>&gt;&gt;&gt; }<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; and the precision would be the same as creating an NSDecimalNumber from a<br>&gt;&gt;&gt; String: <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; let a: CurrencyAmount = 1.00000000000000000000000000000000001<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; print(a.value.debugDescription)<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; Would give: 1.00000000000000000000000000000000001<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; <br>&gt;&gt;&gt; How does that sound? Is it completely irrational to allow the use of Strings as<br>&gt;&gt;&gt; the intermediary representation of float literals?<br>&gt;&gt;&gt; I think that it makes good sense, since it allows for arbitrary precision.<br>&gt;&gt;&gt; <br>&gt;&gt;&gt; Please let me know what you think.<br>&gt;&gt; <br>&gt;&gt; Like Dmitri said, a String is not a particularly efficient intermediate representation. For common machine numeric types, we want it to be straightforward for the compiler to constant-fold literals down to constants in the resulting binary. For floating-point literals, I think we could achieve this by changing the protocol to &quot;deconstruct&quot; the literal value into integer significand and exponent, something like this:<br>&gt;&gt; <br>&gt;&gt; // A type that can be initialized from a decimal literal such as<br>&gt;&gt; // `1.1` or `2.3e5`.<br>&gt;&gt; protocol DecimalLiteralConvertible {<br>&gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;&gt; <br>&gt;&gt;  // Construct a value equal to `decimalSignificand * 10**decimalExponent`.<br>&gt;&gt;  init(decimalSignificand: Component, decimalExponent: Component)<br>&gt;&gt; }<br>&gt;&gt; <br>&gt;&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt;&gt; // literal, such as `0x1.8p-2`.<br>&gt;&gt; protocol HexFloatLiteralConvertible {<br>&gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;&gt; <br>&gt;&gt;  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent`.<br>&gt;&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component)<br>&gt;&gt; }<br>&gt;&gt; <br>&gt;&gt; Literals would desugar to constructor calls as follows:<br>&gt;&gt; <br>&gt;&gt; 1.0 // T(decimalSignificand: 1, decimalExponent: 0)<br>&gt;&gt; 0.123 // T(decimalSignificand: 123, decimalExponent: -3)<br>&gt;&gt; 1.23e-2 // same<br>&gt;&gt; <br>&gt;&gt; 0x1.8p-2 // T(hexadecimalSignificand: 0x18, binaryExponent: -6)<br>&gt; <br>&gt; This seems like a very good approach to me.<br></p><p>It occurs to me that &quot;sign&quot; probably needs to be an independent parameter, to be able to accurately capture literal -0 and 0:<br></p><p>// A type that can be initialized from a decimal literal such as<br>// `1.1` or `-2.3e5`.<br>protocol DecimalLiteralConvertible {<br> // The integer type used to represent the significand and exponent of the value.<br> typealias Component: IntegerLiteralConvertible<br></p><p> // Construct a value equal to `decimalSignificand * 10**decimalExponent * (isNegative ? -1 : 1)`.<br> init(decimalSignificand: Component, decimalExponent: Component, isNegative: Bool)<br>}<br></p><p>// A type that can be initialized from a hexadecimal floating point<br>// literal, such as `0x1.8p-2`.<br>protocol HexFloatLiteralConvertible {<br> // The integer type used to represent the significand and exponent of the value.<br> typealias Component: IntegerLiteralConvertible<br></p><p> // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent * (isNegative ? -1 : 1)`.<br> init(hexadecimalSignificand: Component, binaryExponent: Component, isNegative: Bool)<br>}<br></p><p>-Joe<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/b2f47d083afa4931ca8897e95f0cb4ab?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Morten Bek Ditlevsen</string> &lt;bek at termestrup.dk&gt;<p>May  9, 2016 at 05:00:00am</p></header><div class="content"><p>This would be an excellent solution to the issue.<br>Do you know if there are any existing plans for something like the<br>DecimalLiteralConvertible?<br></p><p>Another thought:<br>Would it make sense to have the compiler warn about float literal precision<br>issues?<br>Initialization of two different variables with the exact same literal value<br>could yield different precision results if one had a FloatLiteralType<br>aliased to Float80 and the other aliased to Float.<br></p><p><br>On Fri, May 6, 2016 at 6:46 PM Joe Groff &lt;jgroff at apple.com&gt; wrote:<br></p><p>&gt;<br>&gt; &gt; On May 6, 2016, at 9:42 AM, Stephen Canon &lt;scanon at apple.com&gt; wrote:<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;&gt; On May 6, 2016, at 12:41 PM, Joe Groff via swift-evolution &lt;<br>&gt; swift-evolution at swift.org&gt; wrote:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; On May 6, 2016, at 2:24 AM, Morten Bek Ditlevsen via swift-evolution &lt;<br>&gt; swift-evolution at swift.org&gt; wrote:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Currently, in order to conform to FloatLiteralConvertible you need to<br>&gt; implement<br>&gt; &gt;&gt;&gt; an initializer accepting a floatLiteral of the typealias:<br>&gt; FloatLiteralType.<br>&gt; &gt;&gt;&gt; However, this typealias can only be Double, Float, Float80 and other<br>&gt; built-in<br>&gt; &gt;&gt;&gt; floating point types (to be honest, I do not know the exact limitation<br>&gt; since I have<br>&gt; &gt;&gt;&gt; not been able to read find this in the documentation).<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; These floating point types have precision limitations that are not<br>&gt; necessarily<br>&gt; &gt;&gt;&gt; present in the type that you are making FloatLiteralConvertible.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Let’s imagine a CurrencyAmount type that uses an NSDecimalNumber as the<br>&gt; &gt;&gt;&gt; representation of the value:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; public struct CurrencyAmount {<br>&gt; &gt;&gt;&gt; public let value: NSDecimalNumber<br>&gt; &gt;&gt;&gt; // .. other important currency-related stuff ..<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible {<br>&gt; &gt;&gt;&gt; public typealias FloatLiteralType = Double<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt; &gt;&gt;&gt;   print(amount.debugDescription)<br>&gt; &gt;&gt;&gt;   value = NSDecimalNumber(double: amount)<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; let a: CurrencyAmount = 99.99<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; The printed value inside the initializer is 99.989999999999995 - so<br>&gt; the value<br>&gt; &gt;&gt;&gt; has lost precision already in the intermediary Double representation.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; I know that there is also an issue with the NSDecimalNumber double<br>&gt; initializer,<br>&gt; &gt;&gt;&gt; but this is not the issue that we are seeing here.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; One suggestion for a solution to this issue would be to allow the<br>&gt; &gt;&gt;&gt; FloatLiteralType to be aliased to a String.  In this case the compiler<br>&gt; should<br>&gt; &gt;&gt;&gt; parse the float literal token: 99.99 to a String and use that as input<br>&gt; for the<br>&gt; &gt;&gt;&gt; FloatLiteralConvertible initializer.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; This would mean that arbitrary literal precisions are allowed for<br>&gt; &gt;&gt;&gt; FloatLiteralConvertibles that implement their own parsing of a String<br>&gt; value.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; For instance, if the CurrencyAmount used a FloatLiteralType aliased to<br>&gt; String we<br>&gt; &gt;&gt;&gt; would have:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible {<br>&gt; &gt;&gt;&gt; public typealias FloatLiteralType = String<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt; &gt;&gt;&gt;   value = NSDecimalNumber(string: amount)<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; and the precision would be the same as creating an NSDecimalNumber<br>&gt; from a<br>&gt; &gt;&gt;&gt; String:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; let a: CurrencyAmount = 1.00000000000000000000000000000000001<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; print(a.value.debugDescription)<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Would give: 1.00000000000000000000000000000000001<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; How does that sound? Is it completely irrational to allow the use of<br>&gt; Strings as<br>&gt; &gt;&gt;&gt; the intermediary representation of float literals?<br>&gt; &gt;&gt;&gt; I think that it makes good sense, since it allows for arbitrary<br>&gt; precision.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Please let me know what you think.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Like Dmitri said, a String is not a particularly efficient intermediate<br>&gt; representation. For common machine numeric types, we want it to be<br>&gt; straightforward for the compiler to constant-fold literals down to<br>&gt; constants in the resulting binary. For floating-point literals, I think we<br>&gt; could achieve this by changing the protocol to &quot;deconstruct&quot; the literal<br>&gt; value into integer significand and exponent, something like this:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; // A type that can be initialized from a decimal literal such as<br>&gt; &gt;&gt; // `1.1` or `2.3e5`.<br>&gt; &gt;&gt; protocol DecimalLiteralConvertible {<br>&gt; &gt;&gt;  // The integer type used to represent the significand and exponent of<br>&gt; the value.<br>&gt; &gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;  // Construct a value equal to `decimalSignificand *<br>&gt; 10**decimalExponent`.<br>&gt; &gt;&gt;  init(decimalSignificand: Component, decimalExponent: Component)<br>&gt; &gt;&gt; }<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt; &gt;&gt; // literal, such as `0x1.8p-2`.<br>&gt; &gt;&gt; protocol HexFloatLiteralConvertible {<br>&gt; &gt;&gt;  // The integer type used to represent the significand and exponent of<br>&gt; the value.<br>&gt; &gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;  // Construct a value equal to `hexadecimalSignificand *<br>&gt; 2**binaryExponent`.<br>&gt; &gt;&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component)<br>&gt; &gt;&gt; }<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Literals would desugar to constructor calls as follows:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; 1.0 // T(decimalSignificand: 1, decimalExponent: 0)<br>&gt; &gt;&gt; 0.123 // T(decimalSignificand: 123, decimalExponent: -3)<br>&gt; &gt;&gt; 1.23e-2 // same<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; 0x1.8p-2 // T(hexadecimalSignificand: 0x18, binaryExponent: -6)<br>&gt; &gt;<br>&gt; &gt; This seems like a very good approach to me.<br>&gt;<br>&gt; It occurs to me that &quot;sign&quot; probably needs to be an independent parameter,<br>&gt; to be able to accurately capture literal -0 and 0:<br>&gt;<br>&gt; // A type that can be initialized from a decimal literal such as<br>&gt; // `1.1` or `-2.3e5`.<br>&gt; protocol DecimalLiteralConvertible {<br>&gt;  // The integer type used to represent the significand and exponent of the<br>&gt; value.<br>&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;<br>&gt;  // Construct a value equal to `decimalSignificand * 10**decimalExponent *<br>&gt; (isNegative ? -1 : 1)`.<br>&gt;  init(decimalSignificand: Component, decimalExponent: Component,<br>&gt; isNegative: Bool)<br>&gt; }<br>&gt;<br>&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt; // literal, such as `0x1.8p-2`.<br>&gt; protocol HexFloatLiteralConvertible {<br>&gt;  // The integer type used to represent the significand and exponent of the<br>&gt; value.<br>&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;<br>&gt;  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent<br>&gt; * (isNegative ? -1 : 1)`.<br>&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component,<br>&gt; isNegative: Bool)<br>&gt; }<br>&gt;<br>&gt; -Joe<br>-------------- next part --------------<br>An HTML attachment was scrubbed...<br>URL: &lt;https://lists.swift.org/pipermail/swift-evolution/attachments/20160509/e6139d15/attachment.html&gt;<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/06c1dcc7dce6a93d194b9c013376e3f0?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Joe Groff</string> &lt;jgroff at apple.com&gt;<p>May  9, 2016 at 10:00:00am</p></header><div class="content"><p>&gt; On May 8, 2016, at 10:30 PM, Morten Bek Ditlevsen &lt;bek at termestrup.dk&gt; wrote:<br>&gt; <br>&gt; This would be an excellent solution to the issue.<br>&gt; Do you know if there are any existing plans for something like the DecimalLiteralConvertible?<br></p><p>Not that I know of. Someone would have to submit a proposal.<br></p><p>&gt; <br>&gt; Another thought:<br>&gt; Would it make sense to have the compiler warn about float literal precision issues?<br>&gt; Initialization of two different variables with the exact same literal value could yield different precision results if one had a FloatLiteralType aliased to Float80 and the other aliased to Float.<br></p><p>That&#39;s definitely a possibility. We already have machinery in place to raise errors when integer literals overflow Int* types, and we could do something similar for float literals that have excessive precision.<br></p><p>-Joe<br></p><p>&gt; <br>&gt; On Fri, May 6, 2016 at 6:46 PM Joe Groff &lt;jgroff at apple.com&gt; wrote:<br>&gt; <br>&gt; &gt; On May 6, 2016, at 9:42 AM, Stephen Canon &lt;scanon at apple.com&gt; wrote:<br>&gt; &gt;<br>&gt; &gt;<br>&gt; &gt;&gt; On May 6, 2016, at 12:41 PM, Joe Groff via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; On May 6, 2016, at 2:24 AM, Morten Bek Ditlevsen via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Currently, in order to conform to FloatLiteralConvertible you need to implement<br>&gt; &gt;&gt;&gt; an initializer accepting a floatLiteral of the typealias: FloatLiteralType.<br>&gt; &gt;&gt;&gt; However, this typealias can only be Double, Float, Float80 and other built-in<br>&gt; &gt;&gt;&gt; floating point types (to be honest, I do not know the exact limitation since I have<br>&gt; &gt;&gt;&gt; not been able to read find this in the documentation).<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; These floating point types have precision limitations that are not necessarily<br>&gt; &gt;&gt;&gt; present in the type that you are making FloatLiteralConvertible.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Let’s imagine a CurrencyAmount type that uses an NSDecimalNumber as the<br>&gt; &gt;&gt;&gt; representation of the value:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; public struct CurrencyAmount {<br>&gt; &gt;&gt;&gt; public let value: NSDecimalNumber<br>&gt; &gt;&gt;&gt; // .. other important currency-related stuff ..<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible {<br>&gt; &gt;&gt;&gt; public typealias FloatLiteralType = Double<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt; &gt;&gt;&gt;   print(amount.debugDescription)<br>&gt; &gt;&gt;&gt;   value = NSDecimalNumber(double: amount)<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; let a: CurrencyAmount = 99.99<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; The printed value inside the initializer is 99.989999999999995 - so the value<br>&gt; &gt;&gt;&gt; has lost precision already in the intermediary Double representation.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; I know that there is also an issue with the NSDecimalNumber double initializer,<br>&gt; &gt;&gt;&gt; but this is not the issue that we are seeing here.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; One suggestion for a solution to this issue would be to allow the<br>&gt; &gt;&gt;&gt; FloatLiteralType to be aliased to a String.  In this case the compiler should<br>&gt; &gt;&gt;&gt; parse the float literal token: 99.99 to a String and use that as input for the<br>&gt; &gt;&gt;&gt; FloatLiteralConvertible initializer.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; This would mean that arbitrary literal precisions are allowed for<br>&gt; &gt;&gt;&gt; FloatLiteralConvertibles that implement their own parsing of a String value.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; For instance, if the CurrencyAmount used a FloatLiteralType aliased to String we<br>&gt; &gt;&gt;&gt; would have:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible {<br>&gt; &gt;&gt;&gt; public typealias FloatLiteralType = String<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt; &gt;&gt;&gt;   value = NSDecimalNumber(string: amount)<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt; }<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; and the precision would be the same as creating an NSDecimalNumber from a<br>&gt; &gt;&gt;&gt; String:<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; let a: CurrencyAmount = 1.00000000000000000000000000000000001<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; print(a.value.debugDescription)<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Would give: 1.00000000000000000000000000000000001<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; How does that sound? Is it completely irrational to allow the use of Strings as<br>&gt; &gt;&gt;&gt; the intermediary representation of float literals?<br>&gt; &gt;&gt;&gt; I think that it makes good sense, since it allows for arbitrary precision.<br>&gt; &gt;&gt;&gt;<br>&gt; &gt;&gt;&gt; Please let me know what you think.<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Like Dmitri said, a String is not a particularly efficient intermediate representation. For common machine numeric types, we want it to be straightforward for the compiler to constant-fold literals down to constants in the resulting binary. For floating-point literals, I think we could achieve this by changing the protocol to &quot;deconstruct&quot; the literal value into integer significand and exponent, something like this:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; // A type that can be initialized from a decimal literal such as<br>&gt; &gt;&gt; // `1.1` or `2.3e5`.<br>&gt; &gt;&gt; protocol DecimalLiteralConvertible {<br>&gt; &gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt; &gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;  // Construct a value equal to `decimalSignificand * 10**decimalExponent`.<br>&gt; &gt;&gt;  init(decimalSignificand: Component, decimalExponent: Component)<br>&gt; &gt;&gt; }<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt; &gt;&gt; // literal, such as `0x1.8p-2`.<br>&gt; &gt;&gt; protocol HexFloatLiteralConvertible {<br>&gt; &gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt; &gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; &gt;&gt;<br>&gt; &gt;&gt;  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent`.<br>&gt; &gt;&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component)<br>&gt; &gt;&gt; }<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; Literals would desugar to constructor calls as follows:<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; 1.0 // T(decimalSignificand: 1, decimalExponent: 0)<br>&gt; &gt;&gt; 0.123 // T(decimalSignificand: 123, decimalExponent: -3)<br>&gt; &gt;&gt; 1.23e-2 // same<br>&gt; &gt;&gt;<br>&gt; &gt;&gt; 0x1.8p-2 // T(hexadecimalSignificand: 0x18, binaryExponent: -6)<br>&gt; &gt;<br>&gt; &gt; This seems like a very good approach to me.<br>&gt; <br>&gt; It occurs to me that &quot;sign&quot; probably needs to be an independent parameter, to be able to accurately capture literal -0 and 0:<br>&gt; <br>&gt; // A type that can be initialized from a decimal literal such as<br>&gt; // `1.1` or `-2.3e5`.<br>&gt; protocol DecimalLiteralConvertible {<br>&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; <br>&gt;  // Construct a value equal to `decimalSignificand * 10**decimalExponent * (isNegative ? -1 : 1)`.<br>&gt;  init(decimalSignificand: Component, decimalExponent: Component, isNegative: Bool)<br>&gt; }<br>&gt; <br>&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt; // literal, such as `0x1.8p-2`.<br>&gt; protocol HexFloatLiteralConvertible {<br>&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;  typealias Component: IntegerLiteralConvertible<br>&gt; <br>&gt;  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent * (isNegative ? -1 : 1)`.<br>&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component, isNegative: Bool)<br>&gt; }<br>&gt; <br>&gt; -Joe<br></p></div><ul class="comments"><li class="comment"><div class="avatar"><img src="https://www.gravatar.com/avatar/49f329a0267f5a1773a77017882a82a8?s=50"></div><header><strong>Allow FloatLiteralType in FloatLiteralConvertible to be aliased to String</strong> from <string>Rainer Brockerhoff</string> &lt;rainer at brockerhoff.net&gt;<p>May  9, 2016 at 02:00:00pm</p></header><div class="content"><p>On 5/9/16 14:01, Joe Groff via swift-evolution wrote:<br>&gt; <br>&gt;&gt; On May 8, 2016, at 10:30 PM, Morten Bek Ditlevsen &lt;bek at termestrup.dk&gt; wrote:<br>&gt;&gt;<br>&gt;&gt; This would be an excellent solution to the issue.<br>&gt;&gt; Do you know if there are any existing plans for something like the DecimalLiteralConvertible?<br>&gt; <br>&gt; Not that I know of. Someone would have to submit a proposal.<br>&gt; <br>&gt;&gt;<br>&gt;&gt; Another thought:<br>&gt;&gt; Would it make sense to have the compiler warn about float literal precision issues?<br>&gt;&gt; Initialization of two different variables with the exact same literal value could yield different precision results if one had a FloatLiteralType aliased to Float80 and the other aliased to Float.<br>&gt; <br>&gt; That&#39;s definitely a possibility. We already have machinery in place to raise errors when integer literals overflow Int* types, and we could do something similar for float literals that have excessive precision.<br></p><p>For compilation, it would probably be overkill to show even a warning<br>for non-representable numbers like 0.1 assigned to a binary<br>floating-point type, but perhaps such a warning might be acceptable in a<br>playground?<br></p><p><br>&gt;&gt; On Fri, May 6, 2016 at 6:46 PM Joe Groff &lt;jgroff at apple.com&gt; wrote:<br>&gt;&gt;<br>&gt;&gt;&gt; On May 6, 2016, at 9:42 AM, Stephen Canon &lt;scanon at apple.com&gt; wrote:<br>&gt;&gt;&gt;<br>&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; On May 6, 2016, at 12:41 PM, Joe Groff via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; On May 6, 2016, at 2:24 AM, Morten Bek Ditlevsen via swift-evolution &lt;swift-evolution at swift.org&gt; wrote:<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Currently, in order to conform to FloatLiteralConvertible you need to implement<br>&gt;&gt;&gt;&gt;&gt; an initializer accepting a floatLiteral of the typealias: FloatLiteralType.<br>&gt;&gt;&gt;&gt;&gt; However, this typealias can only be Double, Float, Float80 and other built-in<br>&gt;&gt;&gt;&gt;&gt; floating point types (to be honest, I do not know the exact limitation since I have<br>&gt;&gt;&gt;&gt;&gt; not been able to read find this in the documentation).<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; These floating point types have precision limitations that are not necessarily<br>&gt;&gt;&gt;&gt;&gt; present in the type that you are making FloatLiteralConvertible.<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Let’s imagine a CurrencyAmount type that uses an NSDecimalNumber as the<br>&gt;&gt;&gt;&gt;&gt; representation of the value:<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; public struct CurrencyAmount {<br>&gt;&gt;&gt;&gt;&gt; public let value: NSDecimalNumber<br>&gt;&gt;&gt;&gt;&gt; // .. other important currency-related stuff ..<br>&gt;&gt;&gt;&gt;&gt; }<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible {<br>&gt;&gt;&gt;&gt;&gt; public typealias FloatLiteralType = Double<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt;&gt;&gt;&gt;&gt;   print(amount.debugDescription)<br>&gt;&gt;&gt;&gt;&gt;   value = NSDecimalNumber(double: amount)<br>&gt;&gt;&gt;&gt;&gt; }<br>&gt;&gt;&gt;&gt;&gt; }<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; let a: CurrencyAmount = 99.99<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; The printed value inside the initializer is 99.989999999999995 - so the value<br>&gt;&gt;&gt;&gt;&gt; has lost precision already in the intermediary Double representation.<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; I know that there is also an issue with the NSDecimalNumber double initializer,<br>&gt;&gt;&gt;&gt;&gt; but this is not the issue that we are seeing here.<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; One suggestion for a solution to this issue would be to allow the<br>&gt;&gt;&gt;&gt;&gt; FloatLiteralType to be aliased to a String.  In this case the compiler should<br>&gt;&gt;&gt;&gt;&gt; parse the float literal token: 99.99 to a String and use that as input for the<br>&gt;&gt;&gt;&gt;&gt; FloatLiteralConvertible initializer.<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; This would mean that arbitrary literal precisions are allowed for<br>&gt;&gt;&gt;&gt;&gt; FloatLiteralConvertibles that implement their own parsing of a String value.<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; For instance, if the CurrencyAmount used a FloatLiteralType aliased to String we<br>&gt;&gt;&gt;&gt;&gt; would have:<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; extension CurrencyAmount: FloatLiteralConvertible {<br>&gt;&gt;&gt;&gt;&gt; public typealias FloatLiteralType = String<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; public init(floatLiteral amount: FloatLiteralType) {<br>&gt;&gt;&gt;&gt;&gt;   value = NSDecimalNumber(string: amount)<br>&gt;&gt;&gt;&gt;&gt; }<br>&gt;&gt;&gt;&gt;&gt; }<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; and the precision would be the same as creating an NSDecimalNumber from a<br>&gt;&gt;&gt;&gt;&gt; String:<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; let a: CurrencyAmount = 1.00000000000000000000000000000000001<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; print(a.value.debugDescription)<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Would give: 1.00000000000000000000000000000000001<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; How does that sound? Is it completely irrational to allow the use of Strings as<br>&gt;&gt;&gt;&gt;&gt; the intermediary representation of float literals?<br>&gt;&gt;&gt;&gt;&gt; I think that it makes good sense, since it allows for arbitrary precision.<br>&gt;&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;&gt; Please let me know what you think.<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; Like Dmitri said, a String is not a particularly efficient intermediate representation. For common machine numeric types, we want it to be straightforward for the compiler to constant-fold literals down to constants in the resulting binary. For floating-point literals, I think we could achieve this by changing the protocol to &quot;deconstruct&quot; the literal value into integer significand and exponent, something like this:<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; // A type that can be initialized from a decimal literal such as<br>&gt;&gt;&gt;&gt; // `1.1` or `2.3e5`.<br>&gt;&gt;&gt;&gt; protocol DecimalLiteralConvertible {<br>&gt;&gt;&gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;&gt;&gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;  // Construct a value equal to `decimalSignificand * 10**decimalExponent`.<br>&gt;&gt;&gt;&gt;  init(decimalSignificand: Component, decimalExponent: Component)<br>&gt;&gt;&gt;&gt; }<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt;&gt;&gt;&gt; // literal, such as `0x1.8p-2`.<br>&gt;&gt;&gt;&gt; protocol HexFloatLiteralConvertible {<br>&gt;&gt;&gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;&gt;&gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt;  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent`.<br>&gt;&gt;&gt;&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component)<br>&gt;&gt;&gt;&gt; }<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; Literals would desugar to constructor calls as follows:<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; 1.0 // T(decimalSignificand: 1, decimalExponent: 0)<br>&gt;&gt;&gt;&gt; 0.123 // T(decimalSignificand: 123, decimalExponent: -3)<br>&gt;&gt;&gt;&gt; 1.23e-2 // same<br>&gt;&gt;&gt;&gt;<br>&gt;&gt;&gt;&gt; 0x1.8p-2 // T(hexadecimalSignificand: 0x18, binaryExponent: -6)<br>&gt;&gt;&gt;<br>&gt;&gt;&gt; This seems like a very good approach to me.<br>&gt;&gt;<br>&gt;&gt; It occurs to me that &quot;sign&quot; probably needs to be an independent parameter, to be able to accurately capture literal -0 and 0:<br>&gt;&gt;<br>&gt;&gt; // A type that can be initialized from a decimal literal such as<br>&gt;&gt; // `1.1` or `-2.3e5`.<br>&gt;&gt; protocol DecimalLiteralConvertible {<br>&gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;&gt;<br>&gt;&gt;  // Construct a value equal to `decimalSignificand * 10**decimalExponent * (isNegative ? -1 : 1)`.<br>&gt;&gt;  init(decimalSignificand: Component, decimalExponent: Component, isNegative: Bool)<br>&gt;&gt; }<br>&gt;&gt;<br>&gt;&gt; // A type that can be initialized from a hexadecimal floating point<br>&gt;&gt; // literal, such as `0x1.8p-2`.<br>&gt;&gt; protocol HexFloatLiteralConvertible {<br>&gt;&gt;  // The integer type used to represent the significand and exponent of the value.<br>&gt;&gt;  typealias Component: IntegerLiteralConvertible<br>&gt;&gt;<br>&gt;&gt;  // Construct a value equal to `hexadecimalSignificand * 2**binaryExponent * (isNegative ? -1 : 1)`.<br>&gt;&gt;  init(hexadecimalSignificand: Component, binaryExponent: Component, isNegative: Bool)<br>&gt;&gt; }<br>&gt;&gt;<br>&gt;&gt; -Joe<br>&gt; <br>&gt; _______________________________________________<br>&gt; swift-evolution mailing list<br>&gt; swift-evolution at swift.org<br>&gt; https://lists.swift.org/mailman/listinfo/swift-evolution<br>&gt; <br></p><p><br>-- <br>Rainer Brockerhoff  &lt;rainer at brockerhoff.net&gt;<br>Belo Horizonte, Brazil<br>&quot;In the affairs of others even fools are wise<br>In their own business even sages err.&quot;<br>http://brockerhoff.net/blog/<br></p></div></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></div>    </main>
    <script src="/js/app-c283ee129de63ad743722e9511e67a5d.js"></script>
  </body>
  <footer>
    <p>Swift and the Swift logo are trademarks of Apple Inc.</p>
  </footer>
</html>
